
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>1.1. comp neuro</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=62ba249389abaaa9ffc34bf36a076bdc1d65ee18" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=f31d14ad54b65d19161ba51d4ffff3a77ae00456"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="1.2. transfer learning" href="ovw_transfer_learning.html" />
    <link rel="prev" title="1. research_ovws" href="research_ovws.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/logo.png" class="logo" alt="logo">
      
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    overview 👋
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="research_ovws.html">
   1. research_ovws
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     1.1. comp neuro
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ovw_transfer_learning.html">
     1.2. transfer learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ovw_disentanglement.html">
     1.3. disentanglement
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ovw_omics.html">
     1.4. omics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ovw_complexity.html">
     1.5. complexity
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ovw_interesting_science.html">
     1.6. interesting science
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ovw_dl_theory.html">
     1.7. dl theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ovw_scat.html">
     1.8. scattering transform
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ovw_ml_medicine.html">
     1.9. ml in medicine
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ovw_causal_inference.html">
     1.10. causal inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ovw_dl_for_neuro.html">
     1.11. dl for neuro
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ovw_uncertainty.html">
     1.12. uncertainty
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ovw_interp.html">
     1.13. interpretability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ovw_generalization.html">
     1.14. generalization
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../cs/cs.html">
   2. cs
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs/retrieval.html">
     2.1. info retrieval
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs/data_structures.html">
     2.2. data structures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs/languages.html">
     2.3. languages
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs/software.html">
     2.4. software engineering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs/quantum.html">
     2.5. quantum
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs/algo.html">
     2.6. algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs/graphs.html">
     2.7. graphs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs/os.html">
     2.8. os
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs/arch.html">
     2.9. architecture
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs/reproducibility.html">
     2.10. reproducibility
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs/comp_theory.html">
     2.11. cs theory
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../math/math.html">
   3. math
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../math/differential_equations.html">
     3.1. differential equations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../math/proofs.html">
     3.2. proofs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../math/analysis.html">
     3.3. real analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../math/linear_algebra.html">
     3.4. linear algebra
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../math/signals.html">
     3.5. signals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../math/optimization.html">
     3.6. optimization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../math/calculus.html">
     3.7. calculus
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../math/chaos.html">
     3.8. chaos
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../math/math_basics.html">
     3.9. math basics
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../stat/stat.html">
   4. stat
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../stat/graphical_models.html">
     4.1. graphical models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../stat/data_analysis.html">
     4.2. data analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../stat/testing.html">
     4.3. testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../stat/causal_inference.html">
     4.4. causal inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../stat/info_theory.html">
     4.5. info theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../stat/linear_models.html">
     4.6. linear models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../stat/time_series.html">
     4.7. time series
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../stat/game_theory.html">
     4.8. game theory
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ml/ml.html">
   5. ml
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml/kernels.html">
     5.1. kernels
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml/nlp.html">
     5.2. nlp
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml/comp_vision.html">
     5.3. computer vision
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml/structure_ml.html">
     5.4. structure learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml/classification.html">
     5.5. classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml/unsupervised.html">
     5.6. unsupervised
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml/deep_learning.html">
     5.7. deep learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml/feature_selection.html">
     5.8. feature selection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml/learning_theory.html">
     5.9. learning theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml/evaluation.html">
     5.10. evaluation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ai/ai.html">
   6. ai
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ai/search.html">
     6.1. search
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ai/decisions_rl.html">
     6.2. decisions, rl
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ai/fairness_sts.html">
     6.3. fairness, sts
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ai/cogsci.html">
     6.4. cogsci
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ai/ai_futures.html">
     6.5. ai futures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ai/logic.html">
     6.6. logic
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ai/philosophy.html">
     6.7. philosophy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ai/psychology.html">
     6.8. psychology
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ai/knowledge_rep.html">
     6.9. representations
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../neuro/neuro.html">
   7. neuro
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../neuro/disease.html">
     7.1. disease
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../neuro/brain_basics.html">
     7.2. brain basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../neuro/vissci.html">
     7.3. vision
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../neuro/comp_neuro.html">
     7.4. comp neuro
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../neuro/sensory_input.html">
     7.5. sensory input
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../neuro/memory.html">
     7.6. memory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../neuro/motor.html">
     7.7. motor system
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../neuro/development.html">
     7.8. development
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>
<a href="https://github.com/csinva/csinva.github.io"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="bottom"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/notes/research_ovws/ovw_comp_neuro.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#navigation">
   1.1.1. navigation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#high-dimensional-computing">
   1.1.2. high-dimensional computing
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#definitions">
     1.1.2.1. definitions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ex-identify-the-language">
     1.1.2.2. ex. identify the language
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#details">
     1.1.2.3. details
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#papers">
     1.1.2.4. papers
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dnns-with-memory">
   1.1.3. dnns with memory
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#visual-sampling">
   1.1.4. visual sampling
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dynamic-routing-between-capsules">
   1.1.5. dynamic routing between capsules
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#hierarchical-temporal-memory-htm">
   1.1.6. hierarchical temporal memory (htm)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#necortical-structure">
     1.1.6.1. necortical structure
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#principles">
     1.1.6.2. principles
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     1.1.6.3. papers
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#forgetting">
   1.1.7. forgetting
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#deeptune-style">
   1.1.8. deeptune-style
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#population-coding">
   1.1.9. population coding
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#interesting-misc-papers">
   1.1.10. interesting misc papers
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>comp neuro</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#navigation">
   1.1.1. navigation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#high-dimensional-computing">
   1.1.2. high-dimensional computing
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#definitions">
     1.1.2.1. definitions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ex-identify-the-language">
     1.1.2.2. ex. identify the language
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#details">
     1.1.2.3. details
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#papers">
     1.1.2.4. papers
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dnns-with-memory">
   1.1.3. dnns with memory
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#visual-sampling">
   1.1.4. visual sampling
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dynamic-routing-between-capsules">
   1.1.5. dynamic routing between capsules
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#hierarchical-temporal-memory-htm">
   1.1.6. hierarchical temporal memory (htm)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#necortical-structure">
     1.1.6.1. necortical structure
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#principles">
     1.1.6.2. principles
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     1.1.6.3. papers
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#forgetting">
   1.1.7. forgetting
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#deeptune-style">
   1.1.8. deeptune-style
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#population-coding">
   1.1.9. population coding
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#interesting-misc-papers">
   1.1.10. interesting misc papers
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="comp-neuro">
<h1><span class="section-number">1.1. </span>comp neuro<a class="headerlink" href="#comp-neuro" title="Permalink to this headline">#</a></h1>
<section id="navigation">
<h2><span class="section-number">1.1.1. </span>navigation<a class="headerlink" href="#navigation" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>cognitive maps (tolman 1940s) - idea that rats in mazes learn spatial maps</p></li>
<li><p><strong>place cells</strong> (o’keefe 1971) - in the hippocampus - fire to indicate one’s current location</p></li>
<li><p>remap to new locations</p></li>
<li><p><strong>grid cells</strong> (moser &amp; moser 2005) - in the entorhinal cotex (provides inputs to the hippocampus) - not particular locations but rather hexagonal coordinate system</p></li>
<li><p>grid cells fire if the mouse is in any location at the vertex (or center) of one of the hexagons</p></li>
<li><p><img alt="Screen Shot 2019-05-10 at 1.25.02 PM" src="../../_images/mouse.png" /></p></li>
<li><p>there are grid cells with larger/smaller hexagons, different orientations, different offsets</p></li>
<li><p>can look for grid cells signature in fmri: <a class="reference external" href="https://www.nature.com/articles/nature08704">https://www.nature.com/articles/nature08704</a></p></li>
<li><p>other places with grid cell-like behavior</p></li>
<li><p>eye movement task</p></li>
<li><p>some evidence for “time cells” like place cells for time</p></li>
<li><p>sound frequency task <a class="reference external" href="https://www.nature.com/articles/nature21692">https://www.nature.com/articles/nature21692</a></p></li>
<li><p>2d “bird space” <a class="reference external" href="https://science.sciencemag.org/content/352/6292/1464.full?ijkey=sXaWNaNjkIcik&amp;keytype=ref&amp;siteid=sci">task</a></p></li>
</ul>
</section>
<section id="high-dimensional-computing">
<h2><span class="section-number">1.1.2. </span>high-dimensional computing<a class="headerlink" href="#high-dimensional-computing" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=82syi1BH_YY">new talk</a> (has slide with related references)</p></li>
<li><p>high-level overview</p>
<ul>
<li><p>current inspiration has all come from single neurons at a time - hd computing is going past this</p></li>
<li><p>the brain’s circuits are high-dimensional</p></li>
<li><p>elements are stochastic not deterministic</p></li>
<li><p>can learn from experience</p></li>
<li><p>no 2 brains are alike yet they exhibit the same behavior</p></li>
</ul>
</li>
<li><p>basic question of comp neuro: what kind of computing can explain behavior produced by spike trains?</p>
<ul>
<li><p>recognizing ppl by how they look, sound, or behave</p></li>
<li><p>learning from examples</p></li>
<li><p>remembering things going back to childhood</p></li>
<li><p>communicating with language</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://link.springer.com/content/pdf/10.1007/s12559-009-9009-8.pdf">HD computing overview paper</a></p>
<ul>
<li><p>in these high dimensions, most points are close to equidistant from one another (L1 distance), and are approximately orthogonal (dot product is 0)</p></li>
<li><p>memory</p>
<ul>
<li><p><em>heteroassociative</em> - can return stored <em>X</em> based on its address <em>A</em></p></li>
<li><p><em>autoassociative</em> - can return stored <em>X</em> based on a noisy version of <em>X</em> (since it is a point attractor), maybe with some iteration</p>
<ul>
<li><p>this adds robustness to the memory</p></li>
<li><p>this also removes the need for addresses altogether</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<section id="definitions">
<h3><span class="section-number">1.1.2.1. </span>definitions<a class="headerlink" href="#definitions" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>what is hd computing?</p>
<ul>
<li><p>compute with random high-dim vectors</p></li>
<li><p>ex. 10k vectors A, B of +1/-1 (also extends to real / complex vectors)</p></li>
</ul>
</li>
<li><p>3 operations</p>
<ul>
<li><p><strong>addition</strong>: A + B = (0, 0, 2, 0, 2,-2, 0,  ….)</p>
<ul>
<li><p>alternatively, could take mean</p></li>
</ul>
</li>
<li><p><strong>multiplication</strong>: A * B =  (-1, -1, -1, 1, 1, -1, 1, …) - this is <strong>XOR</strong></p>
<ul>
<li><p>want this to be invertible, dsitribute over addition, preserve distance, and be dissimilar to the vectors being multiplied</p></li>
<li><p>number of ones after multiplication is the distance between the two original vectors</p></li>
<li><p>can represent a dissimilar set vector by using multiplication</p></li>
</ul>
</li>
<li><p>permutation: shuffles values</p>
<ul>
<li><p>ex. rotate (bit shift with wrapping around)</p></li>
<li><p>multiply by rotation matrix (where each row and col contain exactly one 1)</p></li>
<li><p>can think of permutation as a list of numbers 1, 2, …, n in permuted order</p></li>
<li><p>many properties similar to multiplication</p></li>
<li><p>random permutation randomizes</p></li>
</ul>
</li>
</ul>
</li>
<li><p>basic operations</p>
<ul>
<li><p>weighting by a scalar</p></li>
<li><p>similarity = dot product (sometimes normalized)</p>
<ul>
<li><p>A <span class="math notranslate nohighlight">\(\cdot\)</span> A = 10k</p></li>
<li><p>A <span class="math notranslate nohighlight">\(\cdot\)</span> A = 0 (orthogonal)</p></li>
<li><p>in high-dim spaces, almost all pairs of vectors are dissimilar A <span class="math notranslate nohighlight">\(\cdot\)</span> B = 0</p></li>
<li><p>goal: similar meanings should have large similarity</p></li>
</ul>
</li>
<li><p>normalization</p>
<ul>
<li><p>for binary vectors, just take the sign</p></li>
<li><p>for non-binary vectors, scalar weight</p></li>
</ul>
</li>
</ul>
</li>
<li><p>data structures</p></li>
<li><p>these operations allow for encoding all normal data structures: sets, sequences, lists, databases</p>
<ul>
<li><p>set - can be represented with a sum (since the sum is similar to all the vectors)</p>
<ul>
<li><p>can find a stored set using any element</p></li>
<li><p>if we don’t store the sum, can probe with the sum and keep subtracting the vectors we find</p></li>
</ul>
</li>
<li><p>multiset = bag (stores set with frequency counts) - can store things with order by adding them multiple times, but hard to actually retrieve frequencies</p></li>
<li><p>sequence - could have each element be an address pointing to the next element</p>
<ul>
<li><p>problem - hard to represent sequences that share a subsequence (could have pointers which skip over the subsquence)</p></li>
<li><p>soln: index elements based on permuted sums</p>
<ul>
<li><p>can look up an element based on previous element or previous string of elements</p></li>
</ul>
</li>
<li><p>could do some kind of weighting also</p></li>
</ul>
</li>
<li><p>pairs - could just multiply (XOR), but then get some weird things, e.g. A * A = <strong>0</strong></p>
<ul>
<li><p>instead, permute then multiply</p></li>
<li><p>can use these to index (address, value) pairs and make more complex data structures</p></li>
</ul>
</li>
<li><p>named tuples - have smth like (name: x, date: m, age: y)  and store as holistic vector <span class="math notranslate nohighlight">\(H = N*X + D *  M + A * Y\)</span></p>
<ul>
<li><p>individual attribute value can be retrieved using vector for individual key</p></li>
</ul>
</li>
<li><p>representation substituting is a little trickier….</p>
<ul>
<li><p>we blur what is a value and whit is a variable</p></li>
<li><p>can do this for a pair or for a named tuple with new values</p>
<ul>
<li><p>this doesn’t always work</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>examples</p>
<ul>
<li><p>context vectors</p>
<ul>
<li><p>standard practice (e.g. LSA): make matrix of word counts, where each row is a word, and each column is a document</p></li>
<li><p>HD computing alternative: each row is a word, but each document is assigned a few ~10 columns at random</p>
<ul>
<li><p>thus, the number of columns doesn’t scale with the number of documents</p></li>
<li><p><strong>can also do this randomness for the rows (so the number of rows &lt; the number of words)</strong></p></li>
<li><p>can still get semantic vector for a row/column by adding together the rows/columns which are activated by that row/column</p></li>
<li><p>this examples still only uses bag-of-words (but can be extended to more)</p></li>
</ul>
</li>
</ul>
</li>
<li><p>context vectors 2 (like word2vec)</p>
<ul>
<li><p>each word in vocab is given 2 vectors</p>
<ul>
<li><p>random-indexing vector - fixed random from the beginning</p></li>
<li><p>semantic vector - starts at 0</p></li>
</ul>
</li>
<li><p>as we traverse sequence, for each word, add random-indexing vector from words right before/after it to its semantic vector</p>
<ul>
<li><p>can also permute them before adding to preserve word order</p></li>
</ul>
</li>
</ul>
</li>
<li><p>learning rules by example</p>
<ul>
<li><p>particular instance of a rule is a rule (e.g mother-son-baby <span class="math notranslate nohighlight">\(\to\)</span> grandmother)</p>
<ul>
<li><p>as we get more examples and average them, the rule gets better</p></li>
<li><p>doesn’t always work (especially when things collapse to identity rule)</p></li>
</ul>
</li>
</ul>
</li>
<li><p>analogies from pairs</p>
<ul>
<li><p>ex. what is the dollar of mexico?</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="ex-identify-the-language">
<h3><span class="section-number">1.1.2.2. </span>ex. identify the language<a class="headerlink" href="#ex-identify-the-language" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>paper: <a class="reference external" href="https://arxiv.org/pdf/1412.7026.pdf">LANGUAGE RECOGNITION USING RANDOM INDEXING</a> (joshi et al. 2015)</p></li>
<li><p>benefits - very simple and scalable - only go through data once</p>
<ul>
<li><p>equally easy to use 4-grams vs. 5-grams</p></li>
</ul>
</li>
<li><p>data</p>
<ul>
<li><p>train: given million bytes of text per language (in the same alphabet)</p></li>
<li><p>test: new sentences for each language</p></li>
</ul>
</li>
<li><p>training: compute a 10k profile vector for each language and for each test sentence</p>
<ul>
<li><p>could encode each letter wih a seed vector which is 10k</p></li>
<li><p>instead encode trigrams with <strong>rotate and multiply</strong></p>
<ul>
<li><p>1st letter vec rotated by 2 * 2nd letter vec rotated by 1 * 3rd letter vec</p></li>
<li><p>ex. THE = r(r(T)) * r(H) * r(E)</p></li>
<li><p>approximately orthogonal to all the letter vectors and all the other possible trigram vectors…</p></li>
</ul>
</li>
<li><p>profile = sum of all trigram vectors (taken sliding)</p>
<ul>
<li><p>ex. banana = ban + ana + nan + ana</p></li>
<li><p>profile is like a histogram of trigrams</p></li>
</ul>
</li>
</ul>
</li>
<li><p>testing</p>
<ul>
<li><p>compare each test sentence to profiles via dot product</p></li>
<li><p>clusters similar languages - cool!</p></li>
<li><p>gets 97% test acc</p></li>
<li><p>can query the letter most likely to follor “TH”</p>
<ul>
<li><p>form query vector <span class="math notranslate nohighlight">\(Q = r(r(T)) * r(H)\)</span></p></li>
<li><p>query by using multiply X + Q * english-profile-vec</p></li>
<li><p>find closest letter vecs to X - yields “e”</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="details">
<h3><span class="section-number">1.1.2.3. </span>details<a class="headerlink" href="#details" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>mathematical background</p>
<ul>
<li><p>randomly chosen vecs are dissimilar</p></li>
<li><p>sum vector is similar to its argument vectors</p></li>
<li><p>product vector and permuted vector are dissimilar to their argument vectors</p></li>
<li><p>multiplication distibutes over addition</p></li>
<li><p>permutation distributes over both additions and multiplication</p></li>
<li><p>multiplication and permutations are invertible</p></li>
<li><p>addition is approximately invertible</p></li>
</ul>
</li>
<li><p>comparison to DNNs</p>
<ul>
<li><p>both do statistical learning from data</p></li>
<li><p>data can be noisy</p></li>
<li><p>both use high-dim vecs although DNNs get bad with him dims (e.g. 100k)</p></li>
<li><p>HD is founded on rich mathematical theory</p></li>
<li><p>new codewords are made from existing ones</p></li>
<li><p>HD memory is a separate func</p></li>
<li><p>HD algos are transparent, incremental (on-line), scalable</p></li>
<li><p>somewhat closer to the brain…cerebellum anatomy seems to be match HD</p></li>
<li><p>HD: holistic (distributed repr.) is robust</p></li>
</ul>
</li>
<li><p>different names</p>
<ul>
<li><p>Tony plate: holographic reduced representation</p></li>
<li><p>ross gayler: multiply-add-permute arch</p></li>
<li><p>gayler &amp; levi: vector-symbolic arch</p></li>
<li><p>gallant &amp; okaywe: matrix binding with additive termps</p></li>
<li><p>fourier holographic reduced reprsentations (FHRR; Plate)</p></li>
<li><p>…many more names</p></li>
</ul>
</li>
<li><p>theory of sequence indexing and working memory in RNNs</p>
<ul>
<li><p>trying to make key-value pairs</p></li>
<li><p>VSA as a structured approach for understanding neural networks</p></li>
<li><p>reservoir computing = state-dependent network = echos-state network = liquid state machine - try to represen sequential temporal data - builds representations on the fly</p></li>
</ul>
</li>
</ul>
</section>
<section id="papers">
<h3><span class="section-number">1.1.2.4. </span>papers<a class="headerlink" href="#papers" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://iis-people.ee.ethz.ch/~arahimi/papers/DATE16_HD.pdf">text classification</a> (najafabadi et al. 2016)</p></li>
<li><p><a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/8331890?casa_token=FbderL4T3RgAAAAA:LfP2kRSJwhY5z4OHMqvNDrxmSpyIMLzGs80vGj_IdBXVhVVDwZg1tfIeD2nj0S5N7T2YsRrOcg">Classification and Recall With Binary Hyperdimensional Computing: Tradeoffs in Choice of Density and Mapping Characteristics</a></p>
<ul>
<li><p>note: for sparse vectors, might need some threshold before computing mean (otherwise will have too many zeros)</p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="dnns-with-memory">
<h2><span class="section-number">1.1.3. </span>dnns with memory<a class="headerlink" href="#dnns-with-memory" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>Neural Statistician (Edwards &amp; Storkey, 2016) summarises a dataset by averaging over their embeddings</p></li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/1804.01756.pdf">kanerva machine</a></p>
<ul>
<li><p>like a VAE where the prior is derived from an adaptive memory store</p></li>
</ul>
</li>
</ul>
</section>
<section id="visual-sampling">
<h2><span class="section-number">1.1.4. </span>visual sampling<a class="headerlink" href="#visual-sampling" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/1611.09430">Emergence of foveal image sampling from learning to attend in visual scenes</a> (cheung, weiss, &amp; olshausen, 2017) - using neural attention model, learn a retinal sampling lattice</p>
<ul>
<li><p>can figure out what parts of the input the model focuses on</p></li>
</ul>
</li>
</ul>
</section>
<section id="dynamic-routing-between-capsules">
<h2><span class="section-number">1.1.5. </span>dynamic routing between capsules<a class="headerlink" href="#dynamic-routing-between-capsules" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>hinton 1981 - reference frames require structured representations</p>
<ul>
<li><p>mapping units vote for different orientations, sizes, positions based on basic units</p></li>
<li><p>mapping units <strong>gate the activity</strong> from other types of units - weight is dependent on if mapping is activated</p></li>
<li><p>top-down activations give info back to mapping units</p></li>
<li><p>this is a hopfield net with three-way connections (between input units, output units, mapping units)</p></li>
<li><p>reference frame is a key part of how we see - need to vote for transformations</p></li>
</ul>
</li>
<li><p>olshausen, anderson, &amp; van essen 1993 - dynamic routing circuits</p>
<ul>
<li><p>ran simulations of such things (hinton said it was hard to get simulations to work)</p></li>
<li><p>learn things in object-based reference frames</p></li>
<li><p>inputs -&gt; outputs has weight matrix gated by control</p></li>
</ul>
</li>
<li><p>zeiler &amp; fergus 2013 - visualizing things at intermediate layers - deconv (by dynamic routing)</p>
<ul>
<li><p>save indexes of max pooling (these would be the control neurons)</p></li>
<li><p>when you do deconv, assign max value to these indexes</p></li>
</ul>
</li>
<li><p>arathom 02 - map-seeking circuits</p></li>
<li><p>tenenbaum &amp; freeman 2000 - bilinear models</p>
<ul>
<li><p>trying to separate content + style</p></li>
</ul>
</li>
<li><p>hinton et al 2011 - transforming autoencoders - trained neural net to learn to shift imge</p></li>
<li><p>sabour et al 2017 - dynamic routing between capsules</p>
<ul>
<li><p>units output a vector (represents info about reference frame)</p></li>
<li><p>matrix transforms reference frames between units</p></li>
<li><p>recurrent control units settle on some transformation to identify reference frame</p></li>
</ul>
</li>
<li><p>notes from this <a class="reference external" href="https://towardsdatascience.com/capsule-neural-networks-part-2-what-is-a-capsule-846d5418929f">blog post</a></p>
<ul>
<li><p>problems with cnns</p>
<ul>
<li><p>pooling loses info</p></li>
<li><p>don’t account for spatial relations between image parts</p></li>
<li><p>can’t transfer info to new viewpoints</p></li>
</ul>
</li>
<li><p><strong>capsule</strong> - vector specifying the features of an object (e.g. position, size, orientation, hue texture) and its likelihood</p>
<ul>
<li><p>ex. an “eye” capsule could specify the probability it exists, its position, and its size</p></li>
<li><p>magnitude (i.e. length) of vector represents probability it exists (e.g. there is an eye)</p></li>
<li><p>direction of vector represents the instantiation parameters (e.g. position, size)</p></li>
</ul>
</li>
<li><p>hierarchy</p>
<ul>
<li><p>capsules in later layers are functions of the capsules in lower layers, and since capsule has extra properties can ask questions like “are both eyes similarly sized?”</p>
<ul>
<li><p>equivariance = we can ensure our net is invariant to viewpoints by checking for all similar rotations/transformations in the same amount/direction</p></li>
</ul>
</li>
<li><p>active capsules at one level make predictions for the instantiation parameters of higher-level capsules</p>
<ul>
<li><p>when multiple predictions agree, a higher-level capsule is activated</p></li>
</ul>
</li>
</ul>
</li>
<li><p>steps in a capsule (e.g. one that recognizes faces)</p>
<ul>
<li><p>receives an input vector (e.g. representing eye)</p></li>
<li><p>apply affine transformation - encodes spatial relationships (e.g. between eye and where the face should be)</p></li>
<li><p>applying weighted sum by the C weights, learned by the routing algorithm</p>
<ul>
<li><p>these weights are learned to group similar outputs to make higher-level capsules</p></li>
</ul>
</li>
<li><p>vectors are squashed so their magnitudes are between 0 and 1</p></li>
<li><p>outputs a vector</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="hierarchical-temporal-memory-htm">
<h2><span class="section-number">1.1.6. </span>hierarchical temporal memory (htm)<a class="headerlink" href="#hierarchical-temporal-memory-htm" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>binary synapses and learns by modeling the growth of new synapses and the decay of unused synapses</p></li>
<li><p>separate aspects of brains and neurons that are essential for intelligence from those that depend on brain implementation</p></li>
</ul>
<section id="necortical-structure">
<h3><span class="section-number">1.1.6.1. </span>necortical structure<a class="headerlink" href="#necortical-structure" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>evolution leads to physical/logical hierarchy of brain regions</p></li>
<li><p>neocortex is like a flat sheet</p></li>
<li><p>neocortex regions are similar and do similar computation</p>
<ul>
<li><p>Mountcastle 1978: vision regions are vision becase they receive visual input</p></li>
<li><p>number of regions / connectivity seems to be genetic</p></li>
</ul>
</li>
<li><p>before necortex, brain regions were homogenous: spinal cord, brain stem, basal ganglia, …</p></li>
<li><p><img alt="cortical_columns" src="../../_images/cortical_columns.png" /></p></li>
</ul>
</section>
<section id="principles">
<h3><span class="section-number">1.1.6.2. </span>principles<a class="headerlink" href="#principles" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>common algorithims accross neocortex</p></li>
<li><p>hierarchy</p></li>
<li><p><strong>sparse distributed representations (SDR)</strong> - vectors with thousands of bits, mostly 0s</p>
<ul>
<li><p>bits of representation encode semantic properties</p></li>
</ul>
</li>
<li><p>inputs</p>
<ul>
<li><p>data from the sense</p></li>
<li><p>copy of the motor commands</p>
<ul>
<li><p>“sensory-motor” integration - perception is stable while the eyes move</p></li>
</ul>
</li>
</ul>
</li>
<li><p>patterns are constantly changing</p></li>
<li><p>necortex tries to control old brain regions which control muscles</p></li>
<li><p><strong>learning</strong>: region accepts stream of sensory data + motor commands</p>
<ul>
<li><p>learns of changes in inputs</p></li>
<li><p>ouputs motor commands</p></li>
<li><p>only knows how its output changes its input</p></li>
<li><p>must learn how to control behavior via <em>associative linking</em></p></li>
</ul>
</li>
<li><p>sensory encoders - takes input and turnes it into an SDR</p>
<ul>
<li><p>engineered systems can use non-human senses</p></li>
</ul>
</li>
<li><p>behavior needs to be incorporated fully</p></li>
<li><p>temporal memory - is a memory of sequences</p>
<ul>
<li><p>everything the neocortex does is based on memory and recall of sequences of patterns</p></li>
</ul>
</li>
<li><p>on-line learning</p>
<ul>
<li><p>prediction is compared to what actually happens and forms the basis of learning</p></li>
<li><p>minimize the error of predictions</p></li>
</ul>
</li>
</ul>
</section>
<section id="id1">
<h3><span class="section-number">1.1.6.3. </span>papers<a class="headerlink" href="#id1" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>“A Theory of How Columns in the Neocortex Enable Learning the Structure of the World”</p>
<ul>
<li><p>network model that learns the structure of objects through movement</p></li>
<li><p>object recognition</p>
<ul>
<li><p>over time individual columns integrate changing inputs to recognize complete objects</p></li>
<li><p>through existing lateral connections</p></li>
</ul>
</li>
<li><p>within each column, neocortex is calculating a location representation</p>
<ul>
<li><p>locations relative to each other = <strong>allocentric</strong></p></li>
</ul>
</li>
<li><p>much more motion involved</p></li>
<li><p>multiple columns - integrate spatial inputs - make things fast</p></li>
<li><p>single column - integrate touches over time - represent objects properly</p></li>
</ul>
</li>
<li><p>“Why Neurons Have Thousands of Synapses, A Theory of Sequence Memory in Neocortex”</p>
<ul>
<li><p>learning and recalling sequences of patterns</p></li>
<li><p>neuron with lots of synapses can learn transitions of patterns</p></li>
<li><p>network of these can form robust memory</p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="forgetting">
<h2><span class="section-number">1.1.7. </span>forgetting<a class="headerlink" href="#forgetting" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/pdf/1802.07569.pdf">Continual Lifelong Learning with Neural Networks: A Review</a></p>
<ul>
<li><p>main issues is <em>catastrophic forgetting</em> / <em>stability-plasticity dilemma</em></p></li>
<li><p><img alt="Screen Shot 2020-01-01 at 11.49.32 AM" src="../../_images/forgetting.png" /></p></li>
<li><p>2 types of plasticity</p>
<ul>
<li><p>Hebbian plasticity (Hebb 1949) for positive feedback instability</p></li>
<li><p>compensatory homeostatic plasticity which stabilizes neural activity</p></li>
</ul>
</li>
<li><p>approaches: regularization, dynamic architectures (e.g. add more nodes after each task), memory replay</p></li>
</ul>
</li>
</ul>
</section>
<section id="deeptune-style">
<h2><span class="section-number">1.1.8. </span>deeptune-style<a class="headerlink" href="#deeptune-style" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>ponce_19_evolving_stimuli: <a class="reference external" href="https://www.cell.com/action/showPdf?pii=S0092-8674(19)30391-5">https://www.cell.com/action/showPdf?pii=S0092-8674%2819%2930391-5</a></p></li>
<li><p>bashivan_18_ann_synthesis</p></li>
<li><p><a class="reference external" href="https://papers.nips.cc/paper/6738-adaptive-stimulus-selection-for-optimizing-neural-population-responses.pdf">adept paper</a></p>
<ul>
<li><p>use kernel regression from CNN embedding to calculate distances between preset images</p></li>
<li><p>select preset images</p></li>
<li><p>verified with macaque v4 recording</p></li>
<li><p>currently only study that optimizes firing rates of multiple neurons</p>
<ul>
<li><p>pick next stimulus in closed-loop (“adaptive sampling” = “optimal experimental design”)</p></li>
</ul>
</li>
</ul>
</li>
<li><p>J. Benda, T. Gollisch, C. K. Machens, and A. V. Herz, “From response to stimulus: adaptive sampling in sensory physiology”</p>
<ul>
<li><p>find the smallest number of stimuli needed to fit parameters of a model that predicts the recorded neuron’s activity from the
stimulus</p></li>
<li><p>maximizing firing rates via genetic algorithms</p></li>
<li><p>maximizing firing rate via gradient ascent</p></li>
</ul>
</li>
<li><p>C. DiMattina and K. Zhang,“Adaptive stimulus optimization for sensory systems neuroscience”](<a class="reference external" href="https://www.frontiersin.org/articles/10.3389/fncir.2013.00101/full">https://www.frontiersin.org/articles/10.3389/fncir.2013.00101/full</a>)</p>
<ul>
<li><p>2 general approaches: gradient-based approaches + genetic algorithms</p></li>
<li><p>can put constraints on stimulus space</p></li>
<li><p>stimulus adaptation</p></li>
<li><p>might want iso-response surfaces</p></li>
<li><p>maximally informative stimulus ensembles (Machens, 2002)</p></li>
<li><p>model-fitting: pick to maximize info-gain w/ model params</p></li>
<li><p>using fixed stimulus sets like white noise may be deeply problematic for efforts to identify non-linear hierarchical network models due to continuous parameter confounding (DiMattina and Zhang, 2010)</p></li>
<li><p>use for model selection</p></li>
</ul>
</li>
</ul>
</section>
<section id="population-coding">
<h2><span class="section-number">1.1.9. </span>population coding<a class="headerlink" href="#population-coding" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>saxena_19_pop_cunningham: “Towards the neural population doctrine”</p>
<ul>
<li><p>correlated trial-to-trial variability</p>
<ul>
<li><p>Ni et al. showed that the correlated variability in V4 neurons during attention and learning — processes that have inherently different timescales — robustly decreases</p></li>
<li><p>‘choice’ decoder built on neural activity in the first PC performs as well as one built on the full dataset, suggesting that the relationship of neural variability to behavior lies in a relatively small subspace of the state space.</p></li>
</ul>
</li>
<li><p>decoding</p>
<ul>
<li><p>more neurons only helps if neuron doesn’t lie in span of previous neurons</p></li>
</ul>
</li>
<li><p>encoding</p>
<ul>
<li><p>can train dnn goal-driven or train dnn on the neural responses directly</p></li>
</ul>
</li>
<li><p>testing</p>
<ul>
<li><p>important to be able to test population structure directly</p></li>
</ul>
</li>
</ul>
</li>
<li><p><em>population vector coding</em> - ex. neurons coded for direction sum to get final direction</p></li>
<li><p>reduces uncertainty</p></li>
<li><p><em>correlation coding</em> - correlations betweeen spikes carries extra info</p></li>
<li><p><em>independent-spike coding</em> - each spike is independent of other spikes within the spike train</p></li>
<li><p><em>position coding</em> - want to represent a position</p>
<ul>
<li><p>for grid cells, very efficient</p></li>
</ul>
</li>
<li><p><em>sparse coding</em></p></li>
<li><p>hard when noise between neurons is correlated</p></li>
<li><p>measures of information</p></li>
<li><p>eda</p>
<ul>
<li><p>plot neuron responses</p></li>
<li><p>calc neuron covariances</p></li>
</ul>
</li>
</ul>
</section>
<section id="interesting-misc-papers">
<h2><span class="section-number">1.1.10. </span>interesting misc papers<a class="headerlink" href="#interesting-misc-papers" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>berardino 17 eigendistortions</p>
<ul>
<li><p><strong>Fisher info matrix</strong> under certain assumptions = <span class="math notranslate nohighlight">\(Jacob^TJacob\)</span> (pixels x pixels) where <em>Jacob</em> is the Jacobian matrix for the function f action on the pixels x</p></li>
<li><p>most and least noticeable distortion directions corresponding to the eigenvectors of the Fisher info matrix</p></li>
</ul>
</li>
<li><p>gao_19_v1_repr</p>
<ul>
<li><p>don’t learn from images - v1 repr should come from motion like it does in the real world</p></li>
<li><p>repr</p>
<ul>
<li><p>vector of local content</p></li>
<li><p>matrix of local displacement</p></li>
</ul>
</li>
<li><p>why is this repr nice?</p>
<ul>
<li><p>separate reps of static image content and change due to motion</p></li>
<li><p>disentangled rotations</p></li>
</ul>
</li>
<li><p>learning</p>
<ul>
<li><p>predict next image given current image + displacement field</p></li>
<li><p>predict next image vector given current frame vectors + displacement</p></li>
</ul>
</li>
</ul>
</li>
<li><p>kietzmann_18_dnn_in_neuro_rvw</p></li>
<li><p>friston_10_free_energy</p>
<ul>
<li><p><img alt="friston_free_energy" src="../../_images/friston_free_energy.png" /></p></li>
</ul>
</li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notes/research_ovws"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="research_ovws.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">1. </span>research_ovws</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="ovw_transfer_learning.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">1.2. </span>transfer learning</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Chandan Singh<br/>
  
      &copy; Copyright None.<br/>
    <div class="extra_footer">
      <p>
Many of these images are taken from resources on the web.
</p>
    </div>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>