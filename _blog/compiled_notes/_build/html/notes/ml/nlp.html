
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>5.2. nlp</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=62ba249389abaaa9ffc34bf36a076bdc1d65ee18" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=f31d14ad54b65d19161ba51d4ffff3a77ae00456"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="5.3. computer vision" href="comp_vision.html" />
    <link rel="prev" title="5.1. kernels" href="kernels.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/logo.png" class="logo" alt="logo">
      
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    overview 👋
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../research_ovws/research_ovws.html">
   1. research_ovws
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../research_ovws/ovw_transfer_learning.html">
     1.1. transfer learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../research_ovws/ovw_disentanglement.html">
     1.2. disentanglement
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../research_ovws/ovw_omics.html">
     1.3. omics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../research_ovws/ovw_complexity.html">
     1.4. complexity
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../research_ovws/ovw_interesting_science.html">
     1.5. interesting science
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../research_ovws/ovw_dl_theory.html">
     1.6. dl theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../research_ovws/ovw_scat.html">
     1.7. scattering transform
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../research_ovws/ovw_ml_medicine.html">
     1.8. ml in medicine
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../research_ovws/ovw_transformers.html">
     1.9. transformers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../research_ovws/ovw_causal_inference.html">
     1.10. causal inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../research_ovws/ovw_uncertainty.html">
     1.11. uncertainty
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../research_ovws/ovw_interp.html">
     1.12. interpretability
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../research_ovws/ovw_generalization.html">
     1.13. generalization
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../cs/cs.html">
   2. cs
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs/retrieval.html">
     2.1. info retrieval
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs/data_structures.html">
     2.2. data structures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs/languages.html">
     2.3. languages
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs/software.html">
     2.4. software engineering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs/quantum.html">
     2.5. quantum
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs/algo.html">
     2.6. algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs/graphs.html">
     2.7. graphs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs/os.html">
     2.8. os
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs/arch.html">
     2.9. architecture
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs/reproducibility.html">
     2.10. reproducibility
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../cs/comp_theory.html">
     2.11. cs theory
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../math/math.html">
   3. math
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../math/differential_equations.html">
     3.1. differential equations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../math/proofs.html">
     3.2. proofs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../math/analysis.html">
     3.3. real analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../math/linear_algebra.html">
     3.4. linear algebra
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../math/signals.html">
     3.5. signals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../math/optimization.html">
     3.6. optimization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../math/calculus.html">
     3.7. calculus
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../math/chaos.html">
     3.8. chaos
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../math/math_basics.html">
     3.9. math basics
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../stat/stat.html">
   4. stat
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../stat/graphical_models.html">
     4.1. graphical models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../stat/data_analysis.html">
     4.2. data analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../stat/testing.html">
     4.3. testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../stat/causal_inference.html">
     4.4. causal inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../stat/info_theory.html">
     4.5. info theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../stat/linear_models.html">
     4.6. linear models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../stat/time_series.html">
     4.7. time series
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../stat/game_theory.html">
     4.8. game theory
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="ml.html">
   5. ml
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="kernels.html">
     5.1. kernels
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     5.2. nlp
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="comp_vision.html">
     5.3. computer vision
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="structure_ml.html">
     5.4. structure learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification.html">
     5.5. classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="unsupervised.html">
     5.6. unsupervised
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="deep_learning.html">
     5.7. deep learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="feature_selection.html">
     5.8. feature selection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="learning_theory.html">
     5.9. learning theory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="evaluation.html">
     5.10. evaluation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ai/ai.html">
   6. ai
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ai/search.html">
     6.1. search
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ai/decisions_rl.html">
     6.2. decisions, rl
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ai/fairness_sts.html">
     6.3. fairness, sts
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ai/cogsci.html">
     6.4. cogsci
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ai/ai_futures.html">
     6.5. ai futures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ai/logic.html">
     6.6. logic
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ai/philosophy.html">
     6.7. philosophy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ai/psychology.html">
     6.8. psychology
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ai/knowledge_rep.html">
     6.9. representations
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../neuro/neuro.html">
   7. neuro
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../neuro/disease.html">
     7.1. disease
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../neuro/brain_basics.html">
     7.2. brain basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../neuro/vissci.html">
     7.3. vision
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../neuro/comp_neuro.html">
     7.4. comp neuro
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../neuro/sensory_input.html">
     7.5. sensory input
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../neuro/memory.html">
     7.6. memory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../neuro/motor.html">
     7.7. motor system
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../neuro/development.html">
     7.8. development
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>
<a href="https://github.com/csinva/csinva.github.io"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="bottom"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/notes/ml/nlp.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#benchmarks">
   5.2.1. benchmarks
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tasks">
     5.2.1.1. tasks
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#datasets">
     5.2.1.2. datasets
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#eval-metrics">
     5.2.1.3. eval metrics
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#individual-tasks">
   5.2.2. individual tasks
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tokenization">
     5.2.2.1. tokenization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#token-word-embeddings">
     5.2.2.2. token / word embeddings
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#language-modeling">
     5.2.2.3. language modeling
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#topic-modeling">
     5.2.2.4. topic modeling
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#interpretable-prediction-models">
     5.2.2.5. interpretable prediction models
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#grammar-parse-tree-exraction">
     5.2.2.6. grammar / parse-tree exraction
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>nlp</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#benchmarks">
   5.2.1. benchmarks
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tasks">
     5.2.1.1. tasks
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#datasets">
     5.2.1.2. datasets
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#eval-metrics">
     5.2.1.3. eval metrics
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#individual-tasks">
   5.2.2. individual tasks
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tokenization">
     5.2.2.1. tokenization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#token-word-embeddings">
     5.2.2.2. token / word embeddings
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#language-modeling">
     5.2.2.3. language modeling
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#topic-modeling">
     5.2.2.4. topic modeling
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#interpretable-prediction-models">
     5.2.2.5. interpretable prediction models
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#grammar-parse-tree-exraction">
     5.2.2.6. grammar / parse-tree exraction
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="nlp">
<h1><span class="section-number">5.2. </span>nlp<a class="headerlink" href="#nlp" title="Permalink to this headline">#</a></h1>
<p>Some notes on natural language processing, focused on modern improvements based on deep learning. See also notes in <a class="reference external" href="https://csinva.io/notes/research_ovws/ovw_transformers.html">📌 transformers</a>.</p>
<section id="benchmarks">
<h2><span class="section-number">5.2.1. </span>benchmarks<a class="headerlink" href="#benchmarks" title="Permalink to this headline">#</a></h2>
<section id="tasks">
<h3><span class="section-number">5.2.1.1. </span>tasks<a class="headerlink" href="#tasks" title="Permalink to this headline">#</a></h3>
<p>Nice repo keeping track of progress <a class="reference external" href="https://github.com/sebastianruder/NLP-progress">here</a>.</p>
<ul class="simple">
<li><p>tokenization - convert raw text into tokens that can be fed to a model</p></li>
<li><p>token/word embeddings - convert tokens to semantic vectors</p></li>
<li><p>pos tagging - give each word its part-speech</p></li>
<li><p>named entity recognition - classify tokens based on some known annotations (e.g. person, organization)</p>
<ul>
<li><p>nested entity recognition - more complex than individual tokens (e.g. “Mary = “Jacob’s brother”)</p></li>
</ul>
</li>
<li><p>parse tree extraction - extract out tree of how words operate in a grammar</p></li>
<li><p>sentiment classification</p></li>
<li><p>text summarization</p></li>
<li><p>language modeling (i.e. text generation)</p></li>
<li><p>(machine) translation</p></li>
<li><p>coreference resolution - correspond simple entities (e.g. “she = Mary”, different names for a protein)</p></li>
<li><p>question answering</p></li>
<li><p>natural language inference - determining whether a “hypothesis” is true (entailment), false (contradiction), or undetermined (neutral) given a “premise”</p></li>
<li><p>topic modeling - unsupervised learning over documents</p></li>
</ul>
</section>
<section id="datasets">
<h3><span class="section-number">5.2.1.2. </span>datasets<a class="headerlink" href="#datasets" title="Permalink to this headline">#</a></h3>
<p><strong>open-ended</strong></p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/2009.03300">MMLU</a>: Measuring Massive Multitask Language Understanding (hendrycks, …, song, steinhardt, 2021) - 57 tasks such as elementary math, clinical knowledge, global facts</p></li>
<li><p><a class="reference external" href="https://github.com/allenai/natural-instructions">NLI: allenai natural language instructions</a> (v1: mishra et al. 2022; v2: wang et al. 2022) - 1600+ diverse NLP tasks</p>
<ul>
<li><p>Definition which describes the task</p></li>
<li><p>For each example: (input text, output (varies), explanation*) - explanation only for some tasks</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://github.com/google/BIG-bench">Big-bench</a> (2022) - crowdsourced diverse (but random) NLP tasks</p>
<ul>
<li><p>No task definitions or explanations: the “definition” is instead concatenated to each example’s input</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2206.04301">Unveiling Transformers with LEGO: a synthetic reasoning task</a> (zhang, …, bubeck, …, wagner, 2022) - synthetic reasoning task</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2206.10498">Large Language Models Still Can’t Plan (A Benchmark for LLMs on Planning and Reasoning about Change)</a></p></li>
</ul>
<p><strong>very popular</strong></p>
<ul class="simple">
<li><p><a class="reference external" href="https://super.gluebenchmark.com/">SuperGLUE Benchmark</a> (2019)</p>
<ul>
<li><p>8 NLU tasks (keeps two hardest from GLUE, marked with ***) extending to coreference resolution and question-answering</p></li>
<li><p><a class="reference external" href="https://github.com/google-research-datasets/boolean-questions">BoolQ</a> (Boolean Questions) - answer a yes/no question about a text passage.</p></li>
<li><p><a class="reference external" href="https://github.com/mcdm/CommitmentBank">CB</a> (CommitmentBank) - given a text and a clause, predict how much the text commits to the clause</p></li>
<li><p><a class="reference external" href="https://people.ict.usc.edu/~gordon/copa.html">COPA</a> (Choice of Plausible Alternatives) - given a premise sentence and two possible choices, the system must determine either the cause or effect of the premise from two possible choices</p></li>
<li><p><a class="reference external" href="https://cogcomp.org/multirc/">MultiRC</a> (Multi-sentence Reading Comprehension) - given a context paragraph, a question about that paragraph, and a list of possible answers, the system must predict which answers are true and which are false</p></li>
<li><p><a class="reference external" href="https://sheng-z.github.io/ReCoRD-explorer/">ReCoRD</a> (Reading Comprehension with Commonsense Reasoning Dataset) - multiple-choice QA task (news article + <a class="reference external" href="https://docs.moodle.org/310/en/Embedded_Answers_(Cloze)_question_type">Cloze-style multiple-choice question</a>)</p></li>
<li><p><span class="math notranslate nohighlight">\(^\dagger\)</span><a class="reference external" href="https://aclweb.org/aclwiki/Recognizing_Textual_Entailment">RTE</a> (Recognizing Textual Entailment) - determine if a sentence entails a given hypothesis or not.</p></li>
<li><p><a class="reference external" href="https://pilehvar.github.io/wic/">WiC</a> (Word-in-Context) - word sense disambiguation task cast as binary classification of sentence pairs. Given two text snippets and a polysemous word that appears in both sentences, the task is to determine whether the word is used with the same sense in both sentences</p></li>
<li><p><span class="math notranslate nohighlight">\(^\dagger\)</span><a class="reference external" href="https://cs.nyu.edu/~davise/papers/WinogradSchemas/WS.html">WSC</a> (Winograd Schema Challenge) - coreference resolution task in which examples consist of a sentence with a pronoun and a list of noun phrases from the sentence. The system must determine the correct referent of the pronoun from among the provided choices.</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://gluebenchmark.com/">GLUE Benchmark</a> (2019) = General language understanding evaluation</p>
<ul>
<li><p>9 NLU tasks including question answering, sentiment analysis, text similarity and textual entailment</p></li>
<li><p>single-sentence tasks</p></li>
<li><p><a class="reference external" href="https://nyu-mll.github.io/CoLA/">CoLA</a> (Corpus of Linguistic Acceptability) - determine if sentence is grammatically correct</p></li>
<li><p><a class="reference external" href="https://nlp.stanford.edu/sentiment/index.html">SST-2</a> (Stanford Sentiment Treebank) - determine if the sentence has a positive or negative sentiment</p></li>
<li><p>similarity +  paraphrase tasks</p>
<ul>
<li><p><a class="reference external" href="https://www.microsoft.com/en-us/download/details.aspx?id=52398">MRPC</a> (Microsoft Research Paraphrase Corpus) - determine if two sentences are paraphrases from one another.</p></li>
<li><p><a class="reference external" href="https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs">QQP</a> (Quora Question Pairs)- determine if two questions are semantically equivalent or not.</p></li>
<li><p><a class="reference external" href="http://ixa2.si.ehu.eus/stswiki/index.php/STSbenchmark">STS-B</a> (Semantic Textual Similarity Benchmark) - determine the similarity of two sentences with a score from one to five.</p></li>
</ul>
</li>
<li><p>Inference Tasks</p>
<ul>
<li><p><a class="reference external" href="https://arxiv.org/abs/1704.05426">MNLI</a> (Multi-Genre Natural Language Inference) - determine if a sentence entails, contradicts, or is unrelated to another sentence</p></li>
<li><p><a class="reference external" href="https://rajpurkar.github.io/SQuAD-explorer/">QNLI</a> (Question-answering Natural Language Inference) - determine if the answer to a question is contained in a second sentence or not</p></li>
<li><p><a class="reference external" href="https://aclweb.org/aclwiki/Recognizing_Textual_Entailment">RTE</a> (Recognizing Textual Entailment) - determine if a sentence entails a given hypothesis or not</p></li>
<li><p><a class="reference external" href="https://cs.nyu.edu/~davise/papers/WinogradSchemas/WS.html">WNLI</a> (Winograd Natural Language Inference) - determine if a sentence with an anonymous pronoun and a sentence with this pronoun replaced are entailed or not</p></li>
</ul>
</li>
</ul>
</li>
<li><p>more NLI ( natural language inference)</p>
<ul>
<li><p>ANLI: Adversarial NLI (<a class="reference external" href="https://arxiv.org/abs/1910.14599">nie et al. 2019</a>) - harder examples found by model failures</p></li>
<li><p>SNLI Benchmark (<a class="reference external" href="https://arxiv.org/abs/1508.05326">bowman et al. 2015</a>) = Stanford Natural Languge Inference - entailment dataset</p>
<ul>
<li><p>570k human-annotated sentence pairs where people ask about entailment</p></li>
</ul>
</li>
<li><p>FEVER: Fact Extraction and VERification (<a class="reference external" href="https://aclanthology.org/N18-1074/">Thorne et al., 2018</a>)</p></li>
<li><p>SciTail (<a class="reference external" href="https://ojs.aaai.org/index.php/AAAI/article/view/12022">khot et al. 2018</a>) - textual entailment derived from science-question answering</p></li>
</ul>
</li>
<li><p>QA</p>
<ul>
<li><p>SQuAD 2.0 (<a class="reference external" href="https://arxiv.org/abs/1806.03822">Rajpurkar…liang, 2018</a>) - adds 50k unanswerable questions; system must know when it can’t answer</p></li>
<li><p>SQuAD (<a class="reference external" href="https://arxiv.org/abs/1606.05250">Rajpurkar…liang, 2016</a>) - Stanford Question Answering Dataset (SQuAD)  - 100k questions from 23k passages in 500 wikipedia articles</p></li>
</ul>
</li>
</ul>
<p><strong>common data sources</strong></p>
<ul class="simple">
<li><p>WSJ</p></li>
<li><p>then twitter</p></li>
<li><p>then Wikipedia</p></li>
</ul>
</section>
<section id="eval-metrics">
<h3><span class="section-number">5.2.1.3. </span>eval metrics<a class="headerlink" href="#eval-metrics" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p><strong>perplexity (PP)</strong> - inverse probability of the test set, normalized by the number of words (want to minimize it)</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(PP(W_{test}) = P(w_1, ..., w_N)^{-1/N}\)</span></p></li>
<li><p>can think of this as the weighted average branching factor of a language</p></li>
<li><p>should only be compared across models w/ same vocab</p></li>
</ul>
</li>
<li><p>BLEU</p></li>
<li><p>bert-score: <a class="reference external" href="https://github.com/Tiiiger/bert_score">https://github.com/Tiiiger/bert_score</a></p></li>
</ul>
</section>
</section>
<section id="individual-tasks">
<h2><span class="section-number">5.2.2. </span>individual tasks<a class="headerlink" href="#individual-tasks" title="Permalink to this headline">#</a></h2>
<section id="tokenization">
<h3><span class="section-number">5.2.2.1. </span>tokenization<a class="headerlink" href="#tokenization" title="Permalink to this headline">#</a></h3>
<p><a class="reference external" href="https://huggingface.co/course/chapter2/4?fw=pt">Tokenizers - Hugging Face Course</a></p>
<ul class="simple">
<li><p>word-based</p>
<ul>
<li><p>punctuation splitting</p></li>
<li><p>need to do stemming (e.g. “dog” and “dogs”)</p></li>
<li><p>unknown token [UNK] for anything not seen - to reduce the amount of this, can get character-based tokens</p></li>
<li><p>vocab tends to be too bug</p></li>
</ul>
</li>
<li><p><strong>subword-based</strong> - break apart meaningful subparts of words, most popular (2022)</p>
<ul>
<li><p>many more (e.g. byte-level BPE, used in GPT-2)</p></li>
</ul>
</li>
<li><p>character-based - very little prior, generally sequences are too long</p></li>
<li><p>vocabulary</p>
<ul>
<li><p>sometimes closed, otherwise have unkown words, which we assign its own symbol</p></li>
<li><p>can fix training vocab, or just choose the top words and have the rest be unkown</p></li>
</ul>
</li>
</ul>
</section>
<section id="token-word-embeddings">
<h3><span class="section-number">5.2.2.2. </span>token / word embeddings<a class="headerlink" href="#token-word-embeddings" title="Permalink to this headline">#</a></h3>
<p><strong>embeddings</strong> - vectors for representing words</p>
<ul class="simple">
<li><p>ex. <strong>tf-idf</strong> - defined as counts of nearby words (big + sparse)</p>
<ul>
<li><p>TF * IDF = [ (Number of times term t appears in a document) / (Total number of terms in the document) ] * log(Total number of documents / Number of documents with term t in it).</p></li>
<li><p>pointwise mutual info - instead of counts, consider whether 2 words co-occur more than we would have expected by chance</p></li>
</ul>
</li>
<li><p>ex. <strong>word2vec</strong> - short, dense vectors</p>
<ul>
<li><p>intuition: train classifier on binary prediction: is word <span class="math notranslate nohighlight">\(w\)</span> likely to show up near this word? (algorithm also called skip-gram)</p>
<ul>
<li><p>the weights are the embeddings</p></li>
</ul>
</li>
<li><p>word2vec paper I: <a class="reference external" href="https://arxiv.org/abs/1301.3781">initial word2vec</a> (mikolov et al. 2013) - simplifies neural language models for efficient training of word embeddings</p>
<ul>
<li><p>maximizing the probabilities of words being predicted by their context words (with a DNN)</p></li>
<li><p>continuous bag-of-words (CBOW) - predict current word from window (order doesn’t matter)</p></li>
<li><p>skipgram - use current word to predict surrounding window – nearby context words weighted more heavily</p></li>
</ul>
</li>
<li><p>word2vec paper II: <a class="reference external" href="https://arxiv.org/abs/1310.4546">Distributed Representations of Words and Phrases and their Compositionality</a> (mikolov et al. 2013) - word2vec improvements</p>
<ul>
<li><p>identify key ngrams and give them their own vecs</p></li>
</ul>
</li>
</ul>
</li>
<li><p>ex. <strong>GloVe</strong> (<a class="reference external" href="https://aclanthology.org/D14-1162.pdf">pennington, socher, &amp; manning, 2014</a>), which is based on ratios of word co-occurrence probs</p></li>
<li><p>ex. ELMO (<a class="reference external" href="https://arxiv.org/pdf/1802.05365.pdf">peters…zettlemoyer, 2022</a>) - use LSTM for word embeddings</p></li>
</ul>
</section>
<section id="language-modeling">
<h3><span class="section-number">5.2.2.3. </span>language modeling<a class="headerlink" href="#language-modeling" title="Permalink to this headline">#</a></h3>
<p><strong>language models</strong> - assign probabilities to sequences of words</p>
<ul class="simple">
<li><p>ex. <strong>n-gram model</strong> - assigns probs to short sequences of words, known as n-grams</p>
<ul>
<li><p>for full sentence, use markov assumption</p></li>
</ul>
</li>
<li><p>multi-token decoding for classification - regular beam search will favor shorter results over longer ones on average since a negative log-probability is added at each step, yielding lower (more negative) scores for longer sentences</p></li>
</ul>
</section>
<section id="topic-modeling">
<h3><span class="section-number">5.2.2.4. </span>topic modeling<a class="headerlink" href="#topic-modeling" title="Permalink to this headline">#</a></h3>
<p><strong>topic models (e.g. LDA)</strong> - apply unsupervised learning on large sets of text to learn sets of associated words</p>
<ul class="simple">
<li><p>LDA = latent dirichlet allocation</p></li>
</ul>
</section>
<section id="interpretable-prediction-models">
<h3><span class="section-number">5.2.2.5. </span>interpretable prediction models<a class="headerlink" href="#interpretable-prediction-models" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://ojs.aaai.org/index.php/AAAI/article/view/10954">Neural Bag-of-Ngrams</a> (li et al. 2017) - learn embedding vectors for ngrams via deep version of skip-gram</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1911.10235">Improving N-gram Language Models with Pre-trained Deep Transformer</a> (wang et al. 2019) - use transformer to generate synthetic data for new n-gram model (language model, doesn’t extend to classification)</p>
<ul>
<li><p><a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/8683481?casa_token=7iD-YiGsHTAAAAAA:N3XmuRk27wGttURXYIYDbxdADVdhJMeUeBvVugq0EbyMst-zrm93wPZtc37uUBBtUPXKPrxvGZJC">Improvements to N-gram Language Model Using Text Generated from Neural Language Model</a> (suzuki et al. 2019) - generate synthetic data from RNNs for new n-gram model</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://www.ijcai.org/Proceedings/16/Papers/401.pdf">fasttext</a> (jin et al. 2016)</p></li>
<li><p><a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/9086128">(DirtyCat): Encoding High-Cardinality String Categorical Variables</a> (cerda &amp; varoquax, 2020) - use embedding model to improve string categorical variables</p></li>
</ul>
</section>
<section id="grammar-parse-tree-exraction">
<h3><span class="section-number">5.2.2.6. </span>grammar / parse-tree exraction<a class="headerlink" href="#grammar-parse-tree-exraction" title="Permalink to this headline">#</a></h3>
<p><em>notes/figures from <a class="reference external" href="https://www.nltk.org/book/">nltk book ch 8/9</a></em></p>
<ul class="simple">
<li><p>language -  set of all grammatical sentences</p></li>
<li><p>grammar - formal notation that can be used for “generating” the members of this set</p></li>
<li><p>phrases</p>
<ul>
<li><p>noun phrases</p></li>
<li><p>adjective phrase</p></li>
</ul>
</li>
<li><p>structures</p>
<ul>
<li><p><strong>constituent structure</strong> - words combine with other words to form units which are substitutable</p>
<ul>
<li><p>e.g.  the fact that we can substitute <em>He</em> for <em>The little bear</em> indicates that the latter sequence is a unit</p></li>
</ul>
</li>
<li><p>coordinate structure: if <span class="math notranslate nohighlight">\(v_{1}\)</span> and <span class="math notranslate nohighlight">\(v_{2}\)</span> are both phrases of grammatical category <span class="math notranslate nohighlight">\(X\)</span>, then <span class="math notranslate nohighlight">\(v_{1}\)</span> and <span class="math notranslate nohighlight">\(v_{2}\)</span> is also a phrase of category <span class="math notranslate nohighlight">\(X\)</span></p></li>
</ul>
</li>
<li><p><strong>context-free grammar</strong> - set  of grammar rules that define a gramm</p>
<ul>
<li><p>e.g. <span class="math notranslate nohighlight">\(S \to NP\; VP\)</span></p></li>
<li><p>rules can be probabilities (so we search for the most probable parse tree, rather than returning all)</p></li>
</ul>
</li>
<li><p>phrase tree</p>
<ul>
<li><p><img alt="phrase_tree" src="../../_images/phrase_tree.png" /></p></li>
<li><p><span class="math notranslate nohighlight">\(\begin{array}{lll}\text { Symbol } &amp; \text { Meaning } &amp; \text { Example } \\ \text { S } &amp; \text { sentence } &amp; \text { the man walked } \\ \text { NP } &amp; \text { noun phrase } &amp; \text { a dog } \\ \text { VP } &amp; \text { verb phrase } &amp; \text { saw a park } \\ \text { PP } &amp; \text { prepositional phrase } &amp; \text { with a telescope } \\ \text { Det } &amp; \text { determiner } &amp; \text { the } \\ \text { N }&amp; \text { noun } &amp; \text { dog } \\ \text { V } &amp; \text { verb } &amp; \text { walked } \\ \text{ P} &amp; \text { preposition } &amp; \text { in }\end{array}\)</span></p></li>
<li><p>note: chunking yields a similar partition of a sequence rather than a tree</p></li>
</ul>
</li>
<li><p>algorithms</p>
<ul>
<li><p>top-down parsing (e.g. recursive descent parsing) - starts from <span class="math notranslate nohighlight">\(S\)</span> , and keep expanding the grammar rules until finding a match</p>
<ul>
<li><p>RecursiveDescentParser is unable to handle <strong>left-recursive</strong> productions of the form X -&gt; X Y</p></li>
</ul>
</li>
<li><p>bottom-up parsing - can be faster; find sequences that correspond to the righthand side of a grammar rule and replace them with the left</p>
<ul>
<li><p>ex. shift-reduce parser</p></li>
</ul>
</li>
<li><p>backtracking ensures we find a parse if one exists (and can find multiple if we choose)</p></li>
</ul>
</li>
<li><p><strong>dependency grammar</strong> focuses on how words relate to other words</p>
<ul>
<li><p>relation between <em>head</em> (usually tensed verb) and its <em>dependents</em> (rest of the words)</p></li>
<li><p><img alt="dep_grammar" src="../../_images/dep_grammar.png" /></p></li>
<li><p>this can also be displayed as a tree</p>
<ul>
<li><p><img alt="dep_tree" src="../../_images/dep_tree.png" /></p></li>
</ul>
</li>
<li><p>a dependency graph is <strong>projective</strong> if, when all the words are written in order, the edges can be drawn above the words without crossing</p>
<ul>
<li><p>equivalent to saying that a word and all its descendants (dependents and dependents of its dependents, etc.) form a contiguous sequence of words within the sentence</p></li>
<li><p>above graph is projective</p></li>
</ul>
</li>
<li><p><strong>valencies</strong> - sometimes certain things in a class are allowed buothers are not</p>
<ul>
<li><p>e.g. “Buster was frightened” ✅ but “Buster saw frightened” :x:</p></li>
<li><p>need subcategories of things (e.g. intransitive verb vs transitive verb vs dative verb) to know what symbols are allowed in rules</p></li>
<li><p>verbs specifically have <strong>complements</strong> associated with them – unlike <strong>modifiers</strong> (like the word “really”), complemetes are not optional and usually not selected in the same way by the head</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>feature-based grammars</strong> - add things to word representations (e.g. plurality) and use these in the grammar rules</p></li>
</ul>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notes/ml"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="kernels.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">5.1. </span>kernels</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="comp_vision.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">5.3. </span>computer vision</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Chandan Singh<br/>
  
      &copy; Copyright None.<br/>
    <div class="extra_footer">
      <p>
Many of these images are taken from resources on the web.
</p>
    </div>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>